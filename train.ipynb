{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7569484-9036-44af-b862-1965c99e6014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the actual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "371c380f-539e-4e6a-91a9-5859807c0948",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 08:59:28.947699: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742302768.965466  808873 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742302768.970962  808873 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-18 08:59:28.988634: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow import keras\n",
    "from skimage.feature import hog\n",
    "\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ccef7eb-33fe-4c8b-a7d3-a8b5d11ea7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths for images\n",
    "train_path = 'stanford-cars-dataset/cars_train/cars_train/'\n",
    "test_path = 'stanford-cars-dataset/cars_test/cars_test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b2af1639-cf6a-4ae9-a7e0-1c8026c03e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y2</th>\n",
       "      <th>class</th>\n",
       "      <th>true_class_name</th>\n",
       "      <th>image</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>116</td>\n",
       "      <td>569</td>\n",
       "      <td>375</td>\n",
       "      <td>14</td>\n",
       "      <td>Audi TTS Coupe 2012</td>\n",
       "      <td>00001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>36</td>\n",
       "      <td>116</td>\n",
       "      <td>868</td>\n",
       "      <td>587</td>\n",
       "      <td>3</td>\n",
       "      <td>Acura TL Sedan 2012</td>\n",
       "      <td>00002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>85</td>\n",
       "      <td>109</td>\n",
       "      <td>601</td>\n",
       "      <td>381</td>\n",
       "      <td>91</td>\n",
       "      <td>Dodge Dakota Club Cab 2007</td>\n",
       "      <td>00003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>621</td>\n",
       "      <td>393</td>\n",
       "      <td>1484</td>\n",
       "      <td>1096</td>\n",
       "      <td>134</td>\n",
       "      <td>Hyundai Sonata Hybrid Sedan 2012</td>\n",
       "      <td>00004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14</td>\n",
       "      <td>36</td>\n",
       "      <td>133</td>\n",
       "      <td>99</td>\n",
       "      <td>106</td>\n",
       "      <td>Ford F-450 Super Duty Crew Cab 2012</td>\n",
       "      <td>00005.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8139</th>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>423</td>\n",
       "      <td>336</td>\n",
       "      <td>78</td>\n",
       "      <td>Chrysler Town and Country Minivan 2012</td>\n",
       "      <td>08140.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8140</th>\n",
       "      <td>138</td>\n",
       "      <td>150</td>\n",
       "      <td>706</td>\n",
       "      <td>523</td>\n",
       "      <td>196</td>\n",
       "      <td>smart fortwo Convertible 2012</td>\n",
       "      <td>08141.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8141</th>\n",
       "      <td>26</td>\n",
       "      <td>246</td>\n",
       "      <td>660</td>\n",
       "      <td>449</td>\n",
       "      <td>163</td>\n",
       "      <td>Mercedes-Benz SL-Class Coupe 2009</td>\n",
       "      <td>08142.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8142</th>\n",
       "      <td>78</td>\n",
       "      <td>526</td>\n",
       "      <td>1489</td>\n",
       "      <td>908</td>\n",
       "      <td>112</td>\n",
       "      <td>Ford GT Coupe 2006</td>\n",
       "      <td>08143.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8143</th>\n",
       "      <td>20</td>\n",
       "      <td>240</td>\n",
       "      <td>862</td>\n",
       "      <td>677</td>\n",
       "      <td>17</td>\n",
       "      <td>Audi 100 Sedan 1994</td>\n",
       "      <td>08144.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8144 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       x1   y1    x2    y2  class                         true_class_name  \\\n",
       "0      39  116   569   375     14                     Audi TTS Coupe 2012   \n",
       "1      36  116   868   587      3                     Acura TL Sedan 2012   \n",
       "2      85  109   601   381     91              Dodge Dakota Club Cab 2007   \n",
       "3     621  393  1484  1096    134        Hyundai Sonata Hybrid Sedan 2012   \n",
       "4      14   36   133    99    106     Ford F-450 Super Duty Crew Cab 2012   \n",
       "...   ...  ...   ...   ...    ...                                     ...   \n",
       "8139    3   44   423   336     78  Chrysler Town and Country Minivan 2012   \n",
       "8140  138  150   706   523    196           smart fortwo Convertible 2012   \n",
       "8141   26  246   660   449    163       Mercedes-Benz SL-Class Coupe 2009   \n",
       "8142   78  526  1489   908    112                      Ford GT Coupe 2006   \n",
       "8143   20  240   862   677     17                     Audi 100 Sedan 1994   \n",
       "\n",
       "          image  \n",
       "0     00001.jpg  \n",
       "1     00002.jpg  \n",
       "2     00003.jpg  \n",
       "3     00004.jpg  \n",
       "4     00005.jpg  \n",
       "...         ...  \n",
       "8139  08140.jpg  \n",
       "8140  08141.jpg  \n",
       "8141  08142.jpg  \n",
       "8142  08143.jpg  \n",
       "8143  08144.jpg  \n",
       "\n",
       "[8144 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import the labels into a dataframe\n",
    "path_labels = 'stanford-cars-dataset/stanford_cars_with_class_names.xlsx'\n",
    "\n",
    "if os.path.exists(path_labels):\n",
    "    df_labels = pd.read_excel(path_labels)\n",
    "\n",
    "    df_labels = df_labels.drop('Unnamed: 0', axis=1)\n",
    "    df_labels = df_labels.rename(columns={'ture_class_name': 'true_class_name'})\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfea6be7-eec2-4974-ac0d-bc9630e0a0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_labels['class']\n",
    "image_filenames = df_labels['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0b8c958-2178-4209-9c15-8826f02450ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the path to the image\n",
    "image_filenames = train_path + image_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1097b1ff-33ba-434e-a8ae-2c7523ce0c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "# Get an actual image objects\n",
    "target_size = (320, 240)  # TODO: set target_size how we want it to be.\n",
    "images = []\n",
    "i = 0\n",
    "for filename in image_filenames:\n",
    "    # Read the image\n",
    "    img = cv2.imread(filename)\n",
    "    # Convert to RGB (OpenCV reads images in BGR format)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # TODO: apply the below tomorrow\n",
    "    img = cv2.resize(img, target_size)  # Resize all images to the same dimensions\n",
    "    # add to list\n",
    "    images.append(img)\n",
    "\n",
    "    # making sure all images get processed\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86dbbc98-94fa-4786-8853-4dd8aae73018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracts HOG (Histogram of Oriented Gradients) features form a list of images\n",
    "def extract_hog_features(images):\n",
    "    hog_features = []\n",
    "\n",
    "    i = 0\n",
    "    for image in images:\n",
    "        fd = hog(image, orientations=9, pixels_per_cell=(8, 8), cells_per_block=(2, 2), channel_axis=-1)\n",
    "        # print(f\"Image {i} shape: {image.shape}, HOG feature length: {len(fd)}\")\n",
    "        hog_features.append(fd)\n",
    "\n",
    "        # making sure all images get processed\n",
    "        if i % 500 == 0:\n",
    "            print(i)\n",
    "        i += 1\n",
    "    return np.array(hog_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c47b844d-f6a9-463e-b67a-5e3dee0978ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n"
     ]
    }
   ],
   "source": [
    "# Get features\n",
    "features = extract_hog_features(images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "012a48dd-8e56-4a92-8949-bf63b8dd931c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29228798, 0.3008004 , 0.2193414 , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.03840699, 0.04439234, 0.02844903, ..., 0.1134953 , 0.19899085,\n",
       "        0.11331112],\n",
       "       [0.27152548, 0.03324036, 0.04905377, ..., 0.05895387, 0.19980109,\n",
       "        0.06175043],\n",
       "       ...,\n",
       "       [0.23224188, 0.11655216, 0.06693488, ..., 0.032883  , 0.00927248,\n",
       "        0.00809374],\n",
       "       [0.23231466, 0.06109139, 0.16978195, ..., 0.08451966, 0.0970991 ,\n",
       "        0.31364249],\n",
       "       [0.14970172, 0.09845389, 0.        , ..., 0.04628303, 0.09684546,\n",
       "        0.16841381]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3f90500-0eb5-4fd6-a7ea-ac12a8caf870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features\n",
    "scaler = StandardScaler()\n",
    "normalized_features = scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec806de9-f616-4c15-a768-a695816bafe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(normalized_features, y, test_size=0.2, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2efc6374-a8b2-4fb1-9a25-16d57a8e9186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a simple SVC\n",
    "def train_svc(X_train, y_train):\n",
    "    # Create an SVC model\n",
    "    svc = SVC(kernel='rbf', probability=True)\n",
    "    \n",
    "    # Train the model\n",
    "    svc.fit(X_train, y_train)\n",
    "    \n",
    "    return svc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "511f1d71-5e18-49ee-9ae9-f12a63296773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a neural network to run on the data\n",
    "def train_nn(X_train, y_train, input_dim=40716):\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    model = Sequential([\n",
    "        # Input layer\n",
    "        Dense(input_dim, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.05),\n",
    "        \n",
    "        # Hidden layers\n",
    "        Dense(16384, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.05),\n",
    "        \n",
    "        Dense(8192, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.05),\n",
    "\n",
    "        Dense(8192, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.05),\n",
    "\n",
    "        Dense(4096, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.05),\n",
    "\n",
    "        Dense(2048, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "\n",
    "        Dense(1024, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.1),\n",
    "        \n",
    "        # Output layer\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    # Compile the model\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebf530a7-e532-4f6b-b198-12163b837ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):\n",
    "    # Make predictions\n",
    "    y_pred_proba = model.predict(X_test)\n",
    "    \n",
    "    # Convert probabilities to class labels\n",
    "    if len(y_pred_proba.shape) > 1 and y_pred_proba.shape[1] > 1:\n",
    "        # For multi-class classification\n",
    "        y_pred = np.argmax(y_pred_proba, axis=1)\n",
    "        \n",
    "        # If y_test is one-hot encoded, convert it back to class indices\n",
    "        if len(y_test.shape) > 1 and y_test.shape[1] > 1:\n",
    "            y_test_classes = np.argmax(y_test, axis=1)\n",
    "        else:\n",
    "            y_test_classes = y_test\n",
    "    else:\n",
    "        # For binary classification\n",
    "        y_pred = (y_pred_proba > 0.5).astype(int).flatten()\n",
    "        y_test_classes = y_test\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_test_classes, y_pred)\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    # Detailed classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test_classes, y_pred))\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "59c3f472-398a-4fa5-81ad-44479d9a88b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 09:05:08.066108: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 428ms/step\n",
      "Accuracy: 0.0049\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        13\n",
      "           2       0.00      0.00      0.00         9\n",
      "           3       0.00      0.00      0.00         9\n",
      "           4       0.00      0.00      0.00         9\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.00      0.00      0.00         6\n",
      "           7       0.00      0.00      0.00        12\n",
      "           8       0.00      0.00      0.00        14\n",
      "           9       0.00      0.00      0.00         5\n",
      "          10       0.00      0.00      0.00        12\n",
      "          11       0.00      0.00      0.00         9\n",
      "          12       0.00      0.00      0.00         9\n",
      "          13       0.00      0.00      0.00        10\n",
      "          14       0.00      0.00      0.00         4\n",
      "          15       0.00      0.00      0.00         8\n",
      "          16       0.00      0.00      0.00         3\n",
      "          17       0.00      0.00      0.00         8\n",
      "          18       0.00      0.00      0.00         8\n",
      "          19       0.00      0.00      0.00         9\n",
      "          20       0.00      0.00      0.00        13\n",
      "          21       0.00      0.00      0.00        10\n",
      "          22       0.00      0.00      0.00        11\n",
      "          23       0.00      0.00      0.00         7\n",
      "          24       0.00      0.00      0.00        11\n",
      "          25       0.00      0.00      0.00         7\n",
      "          26       0.00      0.00      0.00         3\n",
      "          27       0.00      0.00      0.00         8\n",
      "          28       0.00      0.00      0.00         2\n",
      "          29       0.00      0.00      0.00        10\n",
      "          30       0.00      0.00      0.00        10\n",
      "          31       0.00      0.00      0.00        11\n",
      "          32       0.00      0.00      0.00         6\n",
      "          33       0.00      0.00      0.00        10\n",
      "          34       0.00      0.00      0.00         8\n",
      "          35       0.00      0.00      0.00         6\n",
      "          36       0.00      0.00      0.00         6\n",
      "          37       0.00      0.00      0.00         9\n",
      "          38       0.00      0.00      0.00         3\n",
      "          39       0.00      0.00      0.00         5\n",
      "          40       0.00      0.00      0.00         4\n",
      "          41       0.00      0.00      0.00         9\n",
      "          42       0.00      0.00      0.00        12\n",
      "          43       0.00      0.00      0.00         6\n",
      "          44       0.00      0.00      0.00         4\n",
      "          45       0.00      0.00      0.00         7\n",
      "          46       0.00      0.00      0.00         8\n",
      "          47       0.00      0.00      0.00         4\n",
      "          48       0.01      1.00      0.01         7\n",
      "          49       0.00      0.00      0.00         7\n",
      "          50       0.00      0.00      0.00         6\n",
      "          51       0.00      0.00      0.00        10\n",
      "          52       0.00      0.00      0.00         8\n",
      "          53       0.00      0.00      0.00         8\n",
      "          54       0.00      0.00      0.00         5\n",
      "          55       0.00      0.00      0.00         6\n",
      "          56       0.00      0.00      0.00        10\n",
      "          57       0.00      0.00      0.00         8\n",
      "          58       0.00      0.00      0.00         8\n",
      "          59       0.00      0.00      0.00         8\n",
      "          60       0.00      0.00      0.00         6\n",
      "          61       0.00      0.00      0.00         8\n",
      "          62       0.00      0.00      0.00         9\n",
      "          63       0.00      0.00      0.00        14\n",
      "          64       0.00      0.00      0.00         7\n",
      "          65       0.00      0.00      0.00        12\n",
      "          66       0.00      0.00      0.00         7\n",
      "          67       0.00      0.00      0.00         4\n",
      "          68       0.00      0.00      0.00         5\n",
      "          69       0.00      0.00      0.00         9\n",
      "          70       0.00      0.00      0.00        10\n",
      "          71       0.00      0.00      0.00        10\n",
      "          72       0.00      0.00      0.00         9\n",
      "          73       0.00      0.00      0.00         5\n",
      "          74       0.00      0.00      0.00        12\n",
      "          75       0.00      0.00      0.00         8\n",
      "          76       0.00      0.00      0.00         7\n",
      "          77       0.00      0.00      0.00        10\n",
      "          78       0.00      0.00      0.00         8\n",
      "          79       0.00      0.00      0.00        15\n",
      "          80       0.00      0.00      0.00        10\n",
      "          81       0.00      0.00      0.00        10\n",
      "          82       0.00      0.00      0.00        13\n",
      "          83       0.00      0.00      0.00         9\n",
      "          84       0.00      0.00      0.00         7\n",
      "          85       0.00      0.00      0.00         7\n",
      "          86       0.00      0.00      0.00         8\n",
      "          87       0.00      0.00      0.00        13\n",
      "          88       0.00      0.00      0.00         8\n",
      "          89       0.00      0.00      0.00         9\n",
      "          90       0.00      0.00      0.00        13\n",
      "          91       0.00      0.00      0.00         4\n",
      "          92       0.00      0.00      0.00         7\n",
      "          93       0.00      0.00      0.00        10\n",
      "          94       0.00      0.00      0.00         8\n",
      "          95       0.00      0.00      0.00         4\n",
      "          96       0.00      0.00      0.00         8\n",
      "          97       0.00      0.00      0.00        12\n",
      "          98       0.00      0.00      0.00         6\n",
      "          99       0.00      0.00      0.00         5\n",
      "         100       0.00      0.00      0.00         4\n",
      "         101       0.00      0.00      0.00        11\n",
      "         102       0.00      0.00      0.00         7\n",
      "         103       0.00      0.00      0.00         8\n",
      "         104       0.00      0.00      0.00         9\n",
      "         105       0.00      0.00      0.00        10\n",
      "         106       0.00      0.00      0.00         7\n",
      "         107       0.00      0.00      0.00        10\n",
      "         108       0.00      0.00      0.00         6\n",
      "         109       0.00      0.00      0.00        13\n",
      "         110       0.00      0.00      0.00        11\n",
      "         111       0.00      0.00      0.00         4\n",
      "         112       0.00      0.00      0.00        11\n",
      "         113       0.00      0.00      0.00        11\n",
      "         114       0.00      0.00      0.00        11\n",
      "         115       0.00      0.00      0.00        10\n",
      "         116       0.00      0.00      0.00         8\n",
      "         117       0.00      0.00      0.00        13\n",
      "         118       0.00      0.00      0.00         9\n",
      "         119       0.00      0.00      0.00        15\n",
      "         120       0.00      0.00      0.00         7\n",
      "         121       0.00      0.00      0.00        10\n",
      "         122       0.00      0.00      0.00         7\n",
      "         123       0.00      0.00      0.00         6\n",
      "         124       0.00      0.00      0.00         5\n",
      "         125       0.00      0.00      0.00         9\n",
      "         126       0.00      0.00      0.00         6\n",
      "         127       0.00      0.00      0.00        12\n",
      "         128       0.00      0.00      0.00        12\n",
      "         129       0.00      0.00      0.00        12\n",
      "         130       0.00      0.00      0.00         6\n",
      "         131       0.00      0.00      0.00         7\n",
      "         132       0.00      0.00      0.00         9\n",
      "         133       0.00      0.00      0.00         8\n",
      "         134       0.00      0.00      0.00         9\n",
      "         135       0.00      0.00      0.00        14\n",
      "         136       0.00      0.00      0.00         4\n",
      "         137       0.00      0.00      0.00         4\n",
      "         138       0.02      0.10      0.03        10\n",
      "         139       0.00      0.00      0.00         7\n",
      "         140       0.00      0.00      0.00         7\n",
      "         141       0.00      0.00      0.00         6\n",
      "         142       0.00      0.00      0.00         7\n",
      "         143       0.00      0.00      0.00         7\n",
      "         144       0.00      0.00      0.00         7\n",
      "         145       0.00      0.00      0.00         9\n",
      "         146       0.00      0.00      0.00         9\n",
      "         147       0.00      0.00      0.00         7\n",
      "         148       0.00      0.00      0.00         7\n",
      "         149       0.00      0.00      0.00         9\n",
      "         150       0.00      0.00      0.00         8\n",
      "         151       0.00      0.00      0.00        10\n",
      "         152       0.00      0.00      0.00         9\n",
      "         153       0.00      0.00      0.00        12\n",
      "         154       0.00      0.00      0.00         4\n",
      "         155       0.00      0.00      0.00         6\n",
      "         156       0.00      0.00      0.00        11\n",
      "         157       0.00      0.00      0.00        11\n",
      "         158       0.00      0.00      0.00         5\n",
      "         159       0.00      0.00      0.00         8\n",
      "         160       0.00      0.00      0.00        12\n",
      "         161       0.00      0.00      0.00         6\n",
      "         162       0.00      0.00      0.00         6\n",
      "         163       0.00      0.00      0.00         5\n",
      "         164       0.00      0.00      0.00         9\n",
      "         165       0.00      0.00      0.00        12\n",
      "         166       0.00      0.00      0.00         8\n",
      "         167       0.00      0.00      0.00         5\n",
      "         168       0.00      0.00      0.00         9\n",
      "         169       0.00      0.00      0.00         7\n",
      "         170       0.00      0.00      0.00         9\n",
      "         171       0.00      0.00      0.00        10\n",
      "         172       0.00      0.00      0.00         8\n",
      "         173       0.00      0.00      0.00        10\n",
      "         174       0.00      0.00      0.00         9\n",
      "         175       0.00      0.00      0.00        10\n",
      "         176       0.00      0.00      0.00        10\n",
      "         177       0.00      0.00      0.00         9\n",
      "         178       0.00      0.00      0.00         5\n",
      "         179       0.00      0.00      0.00         9\n",
      "         180       0.00      0.00      0.00         9\n",
      "         181       0.00      0.00      0.00         8\n",
      "         182       0.00      0.00      0.00        13\n",
      "         183       0.00      0.00      0.00         7\n",
      "         184       0.00      0.00      0.00         8\n",
      "         185       0.00      0.00      0.00         6\n",
      "         186       0.00      0.00      0.00         8\n",
      "         187       0.00      0.00      0.00        10\n",
      "         188       0.00      0.00      0.00         7\n",
      "         189       0.00      0.00      0.00         5\n",
      "         190       0.00      0.00      0.00         8\n",
      "         191       0.00      0.00      0.00         7\n",
      "         192       0.00      0.00      0.00         7\n",
      "         193       0.00      0.00      0.00         9\n",
      "         194       0.00      0.00      0.00         7\n",
      "         195       0.00      0.00      0.00         8\n",
      "         196       0.00      0.00      0.00        10\n",
      "\n",
      "    accuracy                           0.00      1629\n",
      "   macro avg       0.00      0.01      0.00      1629\n",
      "weighted avg       0.00      0.00      0.00      1629\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/fransenq/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/home/fransenq/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/home/fransenq/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([138,  48,  48, ...,  48,  48,  48])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model = train_nn(X_train, y_train, input_dim=40716)\n",
    "evaluate_model(nn_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c5fcde03-222f-4888-b3b3-45258b151903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m204/204\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m87s\u001b[0m 426ms/step\n",
      "Accuracy: 0.0048\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.00      0.00      0.00        32\n",
      "           2       0.00      0.00      0.00        23\n",
      "           3       0.00      0.00      0.00        34\n",
      "           4       0.00      0.00      0.00        33\n",
      "           5       0.00      0.00      0.00        32\n",
      "           6       0.00      0.00      0.00        39\n",
      "           7       0.00      0.00      0.00        27\n",
      "           8       0.00      0.00      0.00        31\n",
      "           9       0.00      0.00      0.00        36\n",
      "          10       0.00      0.00      0.00        21\n",
      "          11       0.00      0.00      0.00        29\n",
      "          12       0.00      0.00      0.00        28\n",
      "          13       0.00      0.00      0.00        31\n",
      "          14       0.00      0.00      0.00        39\n",
      "          15       0.00      0.00      0.00        35\n",
      "          16       0.00      0.00      0.00        41\n",
      "          17       0.00      0.00      0.00        33\n",
      "          18       0.00      0.00      0.00        35\n",
      "          19       0.00      0.00      0.00        32\n",
      "          20       0.00      0.00      0.00        33\n",
      "          21       0.00      0.00      0.00        32\n",
      "          22       0.00      0.00      0.00        32\n",
      "          23       0.00      0.00      0.00        33\n",
      "          24       0.00      0.00      0.00        34\n",
      "          25       0.00      0.00      0.00        33\n",
      "          26       0.00      0.00      0.00        31\n",
      "          27       0.00      0.00      0.00        28\n",
      "          28       0.00      0.00      0.00        39\n",
      "          29       0.00      0.00      0.00        33\n",
      "          30       0.00      0.00      0.00        32\n",
      "          31       0.00      0.00      0.00        33\n",
      "          32       0.00      0.00      0.00        36\n",
      "          33       0.00      0.00      0.00        32\n",
      "          34       0.00      0.00      0.00        37\n",
      "          35       0.00      0.00      0.00        35\n",
      "          36       0.00      0.00      0.00        35\n",
      "          37       0.00      0.00      0.00        30\n",
      "          38       0.00      0.00      0.00        38\n",
      "          39       0.00      0.00      0.00        32\n",
      "          40       0.00      0.00      0.00        35\n",
      "          41       0.00      0.00      0.00        27\n",
      "          42       0.00      0.00      0.00        23\n",
      "          43       0.00      0.00      0.00        40\n",
      "          44       0.00      0.00      0.00        41\n",
      "          45       0.00      0.00      0.00        26\n",
      "          46       0.00      0.00      0.00        36\n",
      "          47       0.00      0.00      0.00        31\n",
      "          48       0.01      0.78      0.01        36\n",
      "          49       0.00      0.00      0.00        31\n",
      "          50       0.00      0.00      0.00        36\n",
      "          51       0.00      0.00      0.00        33\n",
      "          52       0.00      0.00      0.00        33\n",
      "          53       0.00      0.00      0.00        37\n",
      "          54       0.00      0.00      0.00        35\n",
      "          55       0.00      0.00      0.00        34\n",
      "          56       0.00      0.00      0.00        37\n",
      "          57       0.00      0.00      0.00        30\n",
      "          58       0.00      0.00      0.00        36\n",
      "          59       0.00      0.03      0.01        37\n",
      "          60       0.00      0.00      0.00        31\n",
      "          61       0.00      0.00      0.00        35\n",
      "          62       0.00      0.00      0.00        28\n",
      "          63       0.00      0.00      0.00        30\n",
      "          64       0.00      0.00      0.00        23\n",
      "          65       0.00      0.00      0.00        33\n",
      "          66       0.00      0.00      0.00        35\n",
      "          67       0.00      0.00      0.00        35\n",
      "          68       0.00      0.00      0.00        35\n",
      "          69       0.00      0.00      0.00        29\n",
      "          70       0.00      0.00      0.00        33\n",
      "          71       0.00      0.00      0.00        25\n",
      "          72       0.00      0.00      0.00        36\n",
      "          73       0.00      0.00      0.00        40\n",
      "          74       0.00      0.00      0.00        32\n",
      "          75       0.00      0.00      0.00        36\n",
      "          76       0.00      0.00      0.00        37\n",
      "          77       0.00      0.00      0.00        31\n",
      "          78       0.00      0.00      0.00        30\n",
      "          79       0.00      0.00      0.00        34\n",
      "          80       0.00      0.00      0.00        33\n",
      "          81       0.00      0.00      0.00        35\n",
      "          82       0.00      0.00      0.00        32\n",
      "          83       0.00      0.00      0.00        32\n",
      "          84       0.00      0.00      0.00        35\n",
      "          85       0.00      0.00      0.00        37\n",
      "          86       0.00      0.00      0.00        35\n",
      "          87       0.00      0.00      0.00        31\n",
      "          88       0.00      0.00      0.00        32\n",
      "          89       0.00      0.00      0.00        35\n",
      "          90       0.00      0.00      0.00        28\n",
      "          91       0.00      0.00      0.00        35\n",
      "          92       0.00      0.00      0.00        33\n",
      "          93       0.00      0.00      0.00        29\n",
      "          94       0.00      0.00      0.00        36\n",
      "          95       0.00      0.00      0.00        42\n",
      "          96       0.00      0.00      0.00        33\n",
      "          97       0.00      0.00      0.00        30\n",
      "          98       0.00      0.00      0.00        40\n",
      "          99       0.00      0.00      0.00        23\n",
      "         100       0.00      0.00      0.00        30\n",
      "         101       0.00      0.00      0.00        31\n",
      "         102       0.00      0.00      0.00        32\n",
      "         103       0.00      0.00      0.00        32\n",
      "         104       0.00      0.00      0.00        34\n",
      "         105       0.00      0.00      0.00        34\n",
      "         106       0.00      0.00      0.00        35\n",
      "         107       0.00      0.00      0.00        35\n",
      "         108       0.00      0.00      0.00        38\n",
      "         109       0.00      0.00      0.00        32\n",
      "         110       0.00      0.00      0.00        32\n",
      "         111       0.00      0.00      0.00        38\n",
      "         112       0.00      0.00      0.00        35\n",
      "         113       0.00      0.00      0.00        32\n",
      "         114       0.00      0.00      0.00        34\n",
      "         115       0.00      0.00      0.00        35\n",
      "         116       0.00      0.00      0.00        30\n",
      "         117       0.00      0.00      0.00        30\n",
      "         118       0.00      0.00      0.00        33\n",
      "         119       0.00      0.00      0.00        53\n",
      "         120       0.00      0.00      0.00        36\n",
      "         121       0.00      0.00      0.00        35\n",
      "         122       0.00      0.00      0.00        33\n",
      "         123       0.00      0.00      0.00        39\n",
      "         124       0.00      0.00      0.00        34\n",
      "         125       0.00      0.00      0.00        35\n",
      "         126       0.00      0.00      0.00        36\n",
      "         127       0.00      0.00      0.00        29\n",
      "         128       0.00      0.00      0.00        27\n",
      "         129       0.00      0.00      0.00        27\n",
      "         130       0.00      0.00      0.00        35\n",
      "         131       0.00      0.00      0.00        35\n",
      "         132       0.00      0.00      0.00        35\n",
      "         133       0.00      0.00      0.00        34\n",
      "         134       0.00      0.00      0.00        25\n",
      "         135       0.00      0.00      0.00        28\n",
      "         136       0.00      0.05      0.01        20\n",
      "         137       0.00      0.00      0.00        40\n",
      "         138       0.00      0.03      0.01        30\n",
      "         139       0.00      0.00      0.00        36\n",
      "         140       0.00      0.00      0.00        35\n",
      "         141       0.00      0.00      0.00        28\n",
      "         142       0.00      0.00      0.00        26\n",
      "         143       0.00      0.00      0.00        33\n",
      "         144       0.00      0.00      0.00        40\n",
      "         145       0.00      0.00      0.00        35\n",
      "         146       0.00      0.00      0.00        34\n",
      "         147       0.00      0.00      0.00        38\n",
      "         148       0.00      0.00      0.00        38\n",
      "         149       0.00      0.00      0.00        34\n",
      "         150       0.00      0.00      0.00        28\n",
      "         151       0.00      0.00      0.00        34\n",
      "         152       0.00      0.00      0.00        27\n",
      "         153       0.00      0.00      0.00        33\n",
      "         154       0.00      0.00      0.00        39\n",
      "         155       0.00      0.00      0.00        37\n",
      "         156       0.00      0.00      0.00        28\n",
      "         157       0.00      0.00      0.00        26\n",
      "         158       0.00      0.00      0.00        24\n",
      "         159       0.00      0.00      0.00        28\n",
      "         160       0.00      0.00      0.00        32\n",
      "         161       0.00      0.00      0.00        42\n",
      "         162       0.00      0.00      0.00        40\n",
      "         163       0.00      0.00      0.00        32\n",
      "         164       0.00      0.00      0.00        35\n",
      "         165       0.00      0.00      0.00        33\n",
      "         166       0.00      0.00      0.00        33\n",
      "         167       0.00      0.00      0.00        43\n",
      "         168       0.00      0.00      0.00        33\n",
      "         169       0.00      0.00      0.00        32\n",
      "         170       0.00      0.00      0.00        35\n",
      "         171       0.00      0.00      0.00        36\n",
      "         172       0.00      0.00      0.00        36\n",
      "         173       0.00      0.00      0.00        34\n",
      "         174       0.00      0.00      0.00        32\n",
      "         175       0.00      0.00      0.00        21\n",
      "         176       0.00      0.00      0.00        29\n",
      "         177       0.00      0.00      0.00        35\n",
      "         178       0.00      0.00      0.00        37\n",
      "         179       0.00      0.00      0.00        36\n",
      "         180       0.00      0.00      0.00        34\n",
      "         181       0.00      0.00      0.00        30\n",
      "         182       0.00      0.00      0.00        33\n",
      "         183       0.00      0.00      0.00        35\n",
      "         184       0.00      0.00      0.00        33\n",
      "         185       0.00      0.00      0.00        33\n",
      "         186       0.00      0.00      0.00        31\n",
      "         187       0.00      0.00      0.00        34\n",
      "         188       0.00      0.00      0.00        37\n",
      "         189       0.00      0.00      0.00        36\n",
      "         190       0.00      0.00      0.00        35\n",
      "         191       0.00      0.00      0.00        39\n",
      "         192       0.00      0.00      0.00        36\n",
      "         193       0.00      0.00      0.00        33\n",
      "         194       0.00      0.00      0.00        39\n",
      "         195       0.00      0.00      0.00        35\n",
      "         196       0.00      0.00      0.00        30\n",
      "\n",
      "    accuracy                           0.00      6515\n",
      "   macro avg       0.00      0.00      0.00      6515\n",
      "weighted avg       0.00      0.00      0.00      6515\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/home/fransenq/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/home/fransenq/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/mnt/home/fransenq/.local/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([189,  48,  48, ...,  48,  48,  59])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(nn_model, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ac5a8fd-d842-4a3d-9399-b959c7b0968f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.30690867, -0.8215202 , -0.93583995, ..., -1.19493429,\n",
       "        -1.11189386, -0.95318954],\n",
       "       [-1.30690867, -0.8215202 , -0.93583995, ...,  0.93108574,\n",
       "         0.20452542,  0.16514827],\n",
       "       [-0.29954196, -0.2825284 , -0.67266417, ..., -1.19493429,\n",
       "        -0.78883631, -0.25608938],\n",
       "       ...,\n",
       "       [-0.87778408,  0.3090356 ,  0.0641984 , ..., -1.07885411,\n",
       "        -1.07761573, -0.95318954],\n",
       "       [-0.88957855, -0.8215202 , -0.76363142, ...,  1.40138627,\n",
       "         1.79747316, -0.20899359],\n",
       "       [-0.26135943, -0.6192863 , -0.62833609, ..., -0.3826643 ,\n",
       "         0.88243287, -0.78622903]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab3e80db-752d-493e-9cee-7daefce01851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6515, 40716)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb31037d-7af3-4452-919f-db092b505a40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
